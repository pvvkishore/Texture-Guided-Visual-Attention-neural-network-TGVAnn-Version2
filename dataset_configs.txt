# TGVAnn Dataset Configurations

Complete configuration guide for both Maize (TMCI) and Sugarcane (KSCI) datasets.

## ðŸ“Š Dataset Comparison

| Feature | Maize (TMCI) | Sugarcane (KSCI) |
|---------|--------------|------------------|
| **Dataset Name** | TMCI (Texas Maize Crop Images) | KSCI (Kaggle Sugarcane Crop Images) |
| **Number of Classes** | 3 | 5 |
| **Class Names** | Healthy, Disease1, Disease2 | Healthy, Mosaic, RedRot, Rust, Yellow |
| **Recommended Input Size** | 256Ã—256 | 256Ã—256 |
| **Texture Type** | LDP Single-channel | LDP Single-channel |

## âš™ï¸ Training Hyperparameters

### Maize (TMCI) - 3 Classes

```yaml
# configs/maize.yaml
model:
  num_classes: 3
  input_size: 256
  input_channels_rgb: 3
  input_channels_texture: 1

attention:
  num_heads: 1
  d_model: 128
  d_ff: 512
  p_attn: 0.10
  p_ff: 0.10

training:
  batch_size: 32
  epochs: 50
  learning_rate: 1e-4
  weight_decay: 1e-4
  scheduler: cosine
  val_split: 0.2
  augmentation: true
  
optimizer:
  type: adam
  lr: 1e-4
  betas: [0.9, 0.999]
  eps: 1e-8
  weight_decay: 1e-4

data:
  rgb_dir: ./Maize_RGB
  texture_dir: ./Maize_Texture
  num_workers: 4
```

### Sugarcane (KSCI) - 5 Classes

```yaml
# configs/sugarcane.yaml
model:
  num_classes: 5
  input_size: 256
  input_channels_rgb: 3
  input_channels_texture: 1

attention:
  num_heads: 1
  d_model: 128
  d_ff: 512
  p_attn: 0.10
  p_ff: 0.10

training:
  batch_size: 16          # Smaller for 5 classes
  epochs: 100             # More epochs needed
  learning_rate: 1e-5     # Lower LR for better convergence
  weight_decay: 1e-4
  scheduler: cosine
  val_split: 0.2
  augmentation: true
  
optimizer:
  type: adam
  lr: 1e-5                # Lower than Maize
  betas: [0.9, 0.999]
  eps: 1e-8
  weight_decay: 1e-4

data:
  rgb_dir: ./Sugarcane_RGB
  texture_dir: ./Sugarcane_Texture
  num_workers: 4
```

## ðŸŽ¯ Key Differences Explained

### 1. Batch Size
- **Maize**: 32 (fewer classes = larger batches)
- **Sugarcane**: 16 (more classes = smaller batches for better gradients)

### 2. Learning Rate
- **Maize**: 1e-4 (standard for 3-class)
- **Sugarcane**: 1e-5 (lower for fine-grained 5-class discrimination)

### 3. Training Epochs
- **Maize**: 50 (converges faster with 3 classes)
- **Sugarcane**: 100-150 (needs more time for 5-way classification)

### 4. Expected Training Time
- **Maize**: 1-2 hours on GPU
- **Sugarcane**: 2-4 hours on GPU

## ðŸ“ˆ Performance Benchmarks

### Maize (TMCI)

| Metric | Target | Typical |
|--------|--------|---------|
| Overall Accuracy | >95% | 94-98% |
| Macro Precision | >93% | 92-96% |
| Macro Recall | >93% | 91-96% |
| Training Loss (final) | <0.15 | 0.10-0.20 |

### Sugarcane (KSCI)

| Metric | Target | Typical |
|--------|--------|---------|
| Overall Accuracy | >90% | 88-94% |
| Macro Precision | >88% | 87-93% |
| Macro Recall | >87% | 86-92% |
| Training Loss (final) | <0.25 | 0.20-0.35 |

## ðŸ”§ Data Augmentation Settings

### Common for Both Datasets

```python
# RGB Transform (Training)
transforms.Compose([
    transforms.Resize((256, 256)),
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.RandomVerticalFlip(p=0.5),
    transforms.RandomRotation(15),
    transforms.ColorJitter(
        brightness=0.2,
        contrast=0.2,
        saturation=0.2,
        hue=0.1
    ),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]
    )
])

# Texture Transform (Training)
transforms.Compose([
    transforms.Resize((256, 256)),
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.RandomVerticalFlip(p=0.5),
    transforms.RandomRotation(15),
    transforms.Grayscale(num_output_channels=1),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5])
])
```

## ðŸš€ Quick Commands

### Maize (TMCI)

```bash
# Generate textures
python data/ldp_texture_generator.py

# Train
python train.py \
    --rgb_dir ./Maize_RGB \
    --texture_dir ./Maize_Texture \
    --output_dir ./outputs/maize \
    --batch_size 32 \
    --epochs 50 \
    --lr 1e-4 \
    --augment

# Evaluate
python evaluate.py \
    --checkpoint ./outputs/maize/best_model.pth \
    --rgb_dir ./Maize_RGB \
    --texture_dir ./Maize_Texture

# Visualize
python visualize_gradcam.py \
    --checkpoint ./outputs/maize/best_model.pth \
    --rgb_dir ./Maize_RGB \
    --texture_dir ./Maize_Texture \
    --mode both
```

### Sugarcane (KSCI)

```bash
# Generate textures
python generate_texture_sugarcane.py

# Train
python train_sugarcane.py \
    --rgb_dir ./Sugarcane_RGB \
    --texture_dir ./Sugarcane_Texture \
    --output_dir ./outputs/sugarcane \
    --batch_size 16 \
    --epochs 100 \
    --lr 1e-5 \
    --augment

# Evaluate
python evaluate_sugarcane.py \
    --checkpoint ./outputs/sugarcane/best_model_sugarcane.pth \
    --rgb_dir ./Sugarcane_RGB \
    --texture_dir ./Sugarcane_Texture

# Visualize
python visualize_gradcam_sugarcane.py \
    --checkpoint ./outputs/sugarcane/best_model_sugarcane.pth \
    --rgb_dir ./Sugarcane_RGB \
    --texture_dir ./Sugarcane_Texture \
    --mode all
```

## ðŸ“Š Model Architecture Comparison

### Both datasets use identical architecture:

```
Input: RGB (256Ã—256Ã—3) + Texture (256Ã—256Ã—1)
â†“
Conv1 + MaxPool (both streams)
â†“
ResBlock-1 (64Ã—64Ã—64)
â†“
ResBlock-2 (32Ã—32Ã—128)
â†“
TGVA Fusion (32Ã—32Ã—256)
â†“
ResBlock-3 (16Ã—16Ã—256)
â†“
ResBlock-4 (8Ã—8Ã—512)
â†“
Global Avg Pool + FC
â†“
Classifier (3 or 5 classes)
```

**Only difference:** Final classifier layer size
- Maize: 512 â†’ 3
- Sugarcane: 512 â†’ 5

## ðŸ’¾ Model Size & Complexity

| Metric | Maize | Sugarcane |
|--------|-------|-----------|
| Parameters | ~7.2M | ~7.2M |
| FLOPs | ~8.4G | ~8.4G |
| Model Size (FP32) | ~28.8 MB | ~28.8 MB |
| Model Size (FP16) | ~14.4 MB | ~14.4 MB |

*Note: Same model architecture, only classifier differs*

## ðŸŽ“ Transfer Learning

### From Maize to Sugarcane

```python
# Load Maize model (3 classes)
maize_model = TGVAnn(num_classes=3)
maize_checkpoint = torch.load('maize_best.pth')
maize_model.load_state_dict(maize_checkpoint['model_state_dict'])

# Create Sugarcane model (5 classes)
sugarcane_model = TGVAnn(num_classes=5)

# Copy all weights except classifier
pretrained_dict = maize_model.state_dict()
model_dict = sugarcane_model.state_dict()

# Filter out classifier weights
pretrained_dict = {k: v for k, v in pretrained_dict.items() 
                  if 'classifier' not in k}
model_dict.update(pretrained_dict)
sugarcane_model.load_state_dict(model_dict)

# Fine-tune on Sugarcane with lower LR
optimizer = Adam(sugarcane_model.parameters(), lr=1e-6)
```

## ðŸ” Dataset-Specific Notes

### Maize (TMCI)
- âœ… Faster convergence
- âœ… Higher accuracy achievable
- âœ… Fewer class confusion issues
- âš ï¸ May overfit with too many epochs
- ðŸ’¡ Can use larger batch sizes

### Sugarcane (KSCI)
- âœ… More challenging (good for research)
- âœ… Better generalization with augmentation
- âš ï¸ Needs careful hyperparameter tuning
- âš ï¸ Watch for class imbalance
- ðŸ’¡ Benefits from longer training

## ðŸ“ Recommended Workflow

### For Research/Experimentation
1. Start with **Maize** (simpler, faster iteration)
2. Validate approach and architecture
3. Apply to **Sugarcane** with tuned hyperparameters

### For Production
1. Use dataset-specific configurations above
2. Train multiple models with different seeds
3. Ensemble predictions for robustness
4. Monitor per-class performance closely

---

**Both datasets are fully supported with optimized configurations!** ðŸŒ±ðŸŒ¾
